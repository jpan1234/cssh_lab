{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d21d57",
   "metadata": {},
   "source": [
    "## SSC Functions\n",
    "To reduce redundancy, creating and documenting functions here allows for more modularity in the code,\n",
    "\n",
    "Here you will find code based on data cleaning and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d05ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    \"\"\" cleans string of punctuation and unwanted characters\n",
    "    \n",
    "    args:\n",
    "        string(str): string to clean\n",
    "        \n",
    "    returns: \n",
    "        temp(str): cleaned string\n",
    "    \"\"\"\n",
    "    # clean the string\n",
    "    raw_string = str(string)\n",
    "    if type(string) == float:\n",
    "        return \"\"\n",
    "    temp = re.sub(\"'\", \"\", str(string)) # to avoid removing contractions in english\n",
    "    temp = temp.lower()\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub(r\"www.\\S+\", \"\", temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    #temp = re.sub(\"[^a-z0-9]\",\" \", temp) # this removes all letters including CO2\n",
    "    temp = re.sub(\"[^a-z]\",\" \", temp)\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in stopwords]\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    \n",
    "    if temp is None:\n",
    "        print (raw_string,temp)\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b44960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res(string_list):\n",
    "    \"\"\" if a list is in string format ('['hello']'), sets it to its actual type\n",
    "    \n",
    "    args:\n",
    "        string_list(str): list as a string\n",
    "        \n",
    "    \"\"\"\n",
    "    return ast.literal_eval(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_string(series, bad_string):\n",
    "    \"\"\"\n",
    "    removes a string from a series of lists \n",
    "    \n",
    "    args:\n",
    "        series(series): a series\n",
    "        bad_string(str): string to be removed\n",
    "        \n",
    "    returns:\n",
    "        series(series): the series with the removed string\n",
    "    \"\"\"\n",
    "    # Define the remove_re function to remove a specified bad string from all lists\n",
    "    def remove_string(lst, bad_string):\n",
    "        \"\"\" removes string from list\n",
    "        \n",
    "        args:\n",
    "            lst(list): a list\n",
    "            bad_string(str): string to be removed\n",
    "        \"\"\"\n",
    "        # return list after applying removal of string\n",
    "        return [s for s in lst if s != bad_string]\n",
    "    \n",
    "    # return the series after applying the removal of the string\n",
    "    return series.apply(lambda lst: remove_string(lst, bad_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(column_name, dataframe):\n",
    "    \"\"\"\n",
    "    splits a dataframe into a list of dataframes with a given column name \n",
    "    \n",
    "    args:\n",
    "        column_name(str) : the column name string\n",
    "        dataframe(df): the dataframe to be split up\n",
    "        \n",
    "    returns:\n",
    "        dataframes(list): a list of dataframes\n",
    "    \n",
    "    \"\"\"\n",
    "    # create a list of dataframes with list comprehension\n",
    "    dataframes = [group_data for _, group_data in dataframe.groupby(column_name)]\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44886b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LDA(words, num_topics, random_state = 100, update_every = 1,\n",
    "            chunksize = 10, passes = 10, alpha = 'auto',\n",
    "            per_word_topics = True):\n",
    "    \"\"\"\n",
    "    Perform topic modeling using LDA on a series of words.\n",
    "\n",
    "    args:\n",
    "        words (pd.Series): A series of words to have topic modeling imposed on it.\n",
    "        num_topics (int, optional): The number of topics for topic modeling.\n",
    "        random_state (int, optional): Random state for reproducibility. Defaults to 100.\n",
    "        update_every (int, optional): Number of documents to process for each online training iteration. Defaults to 1.\n",
    "        chunksize (int, optional): Number of documents to load into memory at a time for online training. Defaults to 10.\n",
    "        passes (int, optional): Number of passes through the corpus during training. Defaults to 10.\n",
    "        alpha (str or float, optional): The alpha parameter for LDA. 'auto' uses a symmetric distribution. Defaults to 'auto'.\n",
    "        per_word_topics (bool, optional): Whether to include per-word-topic probabilities. Defaults to True.\n",
    "\n",
    "    returns:\n",
    "        vis_data: Data prepared for visualization using pyLDAvis.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize dictionary and corpus with words\n",
    "    dictionary = corpora.Dictionary(words)\n",
    "    corpus = [dictionary.doc2bow(word) for word in words]\n",
    "\n",
    "\n",
    "    # build the LDA model\n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=num_topics,\n",
    "                                       random_state=random_state,\n",
    "                                       update_every=update_every,\n",
    "                                       chunksize=chunksize,\n",
    "                                       passes=passes,\n",
    "                                       alpha=alpha,\n",
    "                                       per_word_topics=per_word_topics)\n",
    "\n",
    "    # visualize the topics using pyLDAvis\n",
    "    vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "    # print the top words for each topic\n",
    "    for topic in lda_model.print_topics():\n",
    "        print(topic)\n",
    "        \n",
    "    return vis_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
